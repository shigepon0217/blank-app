import streamlit as st
import pandas as pd
import numpy as np
import time
import requests

API_TOKEN="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"
API_URL = "https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B"

st.title("ğŸˆ My new StreamLit app")

headers = {
    "Authorization": f"Bearer {API_TOKEN}"
}

# Llamaãƒ¢ãƒ‡ãƒ«ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹é–¢æ•°
def query_llama(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

# Streamlit UIã®è¨­å®š
st.title("Llama Model - Text Generator")
input_text = st.text_area("Enter your prompt:","what is streamlit?")
button = st.button("å›ç­”ã‚’ç”Ÿæˆã™ã‚‹")

if button:
    # Llamaã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡
    with st.spinner("Generating response..."):
        response = query_llama({
            "inputs": input_text,
        })
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
    st.subheader("Generated Response:")
    if 'error' in response:
        st.error(f"Error: {response['error']}")
    else:
        st.write(response[0]["generated_text"])
